#!/usr/bin/env python3
"""
åˆå§‹åŒ– LLM è¨“ç·´å°ˆæ¡ˆ

æ ¹æ“š task_definition.yaml ç”Ÿæˆå®Œæ•´çš„å°ˆæ¡ˆçµæ§‹ï¼Œ
åŒ…å«è…³æœ¬ã€é…ç½®ã€æ–‡ä»¶æ¨¡æ¿ã€‚

Usage:
    python init_project.py --config task_definition.yaml --output tasks/{task_name}
"""

import argparse
import yaml
import json
from pathlib import Path
from datetime import datetime
from string import Template


def load_task_definition(config_path: str) -> dict:
    """è¼‰å…¥ä»»å‹™å®šç¾©"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def create_directory_structure(output_dir: Path):
    """å»ºç«‹ç›®éŒ„çµæ§‹"""
    dirs = [
        'scripts',
        'configs',
        'data/raw',
        'data/chat_format',
        'models/adapter',
        'models/merged',
        'models/gguf',
        'benchmarks/data',
        'benchmarks/results',
        'docs',
        'hf_cards',
        'iterations/v1',
    ]
    for d in dirs:
        (output_dir / d).mkdir(parents=True, exist_ok=True)


def generate_readme(task_def: dict, output_dir: Path):
    """ç”Ÿæˆ README.md"""
    content = f"""# {task_def['task_name']} è¨“ç·´å°ˆæ¡ˆ

## ä»»å‹™æ¦‚è¿°

- **ä»»å‹™é¡å‹**: {task_def['task_type']}
- **é ˜åŸŸ**: {task_def.get('domain', 'é€šç”¨')}
- **èªè¨€**: {task_def.get('language', 'zh-TW')}
- **åŸºç¤æ¨¡å‹**: {task_def.get('training', {}).get('base_model', 'Qwen/Qwen3-4B')}

## å¿«é€Ÿé–‹å§‹

### 1. æº–å‚™è³‡æ–™

å°‡åŸå§‹è³‡æ–™æ”¾å…¥ `data/raw/`ï¼Œæ ¼å¼ç‚º JSONLï¼š

```jsonl
{{"text": "ç¯„ä¾‹æ–‡æœ¬", "label": "æ¨™ç±¤"}}
```

### 2. é©—è­‰å’Œè½‰æ›è³‡æ–™

```bash
python scripts/01_validate_data.py
python scripts/02_convert_format.py
```

### 3. è¨“ç·´æ¨¡å‹

```bash
# æœ¬åœ°è¨“ç·´
python scripts/03_train.py

# æˆ–é ç«¯è¨“ç·´
REMOTE=true ./scripts/run_pipeline.sh
```

### 4. è©•ä¼°

```bash
python scripts/04_evaluate.py
```

### 5. ä¸Šå‚³ HuggingFace

```bash
python scripts/05_upload_hf.py
```

## ä¸€éµåŸ·è¡Œ

```bash
./scripts/run_pipeline.sh
```

## ç›®éŒ„çµæ§‹

```
{task_def['task_name']}/
â”œâ”€â”€ task_definition.yaml      # ä»»å‹™å®šç¾©
â”œâ”€â”€ scripts/                  # å¯åŸ·è¡Œè…³æœ¬
â”œâ”€â”€ configs/                  # è¨“ç·´é…ç½®
â”œâ”€â”€ data/                     # è¨“ç·´è³‡æ–™
â”œâ”€â”€ models/                   # æ¨¡å‹ç”¢å‡º
â”œâ”€â”€ benchmarks/               # è©•ä¼°çµæœ
â”œâ”€â”€ docs/                     # æ–‡ä»¶
â””â”€â”€ hf_cards/                 # HuggingFace Model Cards
```

## ç›®æ¨™æŒ‡æ¨™

| æŒ‡æ¨™ | ç›®æ¨™ |
|------|------|
| {task_def.get('success_criteria', {}).get('primary_metric', 'macro_f1')} | {task_def.get('success_criteria', {}).get('threshold', 0.80):.0%} |

---

*ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M')}*
"""
    (output_dir / 'README.md').write_text(content, encoding='utf-8')


def generate_training_config(task_def: dict, output_dir: Path):
    """ç”Ÿæˆè¨“ç·´é…ç½®"""
    training = task_def.get('training', {})
    lora = training.get('lora', {})

    config = {
        'model': {
            'base_model': training.get('base_model', 'Qwen/Qwen3-4B'),
            'trust_remote_code': True,
        },
        'data': {
            'train_file': 'data/chat_format/train.jsonl',
            'valid_file': 'data/chat_format/valid.jsonl',
        },
        'lora': {
            'r': lora.get('r', 32),
            'lora_alpha': lora.get('alpha', 64),
            'lora_dropout': lora.get('dropout', 0.05),
            'target_modules': [
                'q_proj', 'k_proj', 'v_proj', 'o_proj',
                'gate_proj', 'up_proj', 'down_proj'
            ],
        },
        'training': {
            'method': training.get('method', 'sft'),
            'num_epochs': training.get('epochs', 8),
            'learning_rate': training.get('learning_rate', 1e-5),
            'per_device_train_batch_size': training.get('batch_size', 4),
            'gradient_accumulation_steps': 4,
            'warmup_ratio': 0.1,
            'weight_decay': 0.01,
            'max_seq_length': 2048,
            'logging_steps': 10,
            'save_strategy': 'epoch',
            'evaluation_strategy': 'epoch',
        },
        'output': {
            'output_dir': f"models/adapter",
        },
    }

    config_path = output_dir / 'configs' / 'training_config.yaml'
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, allow_unicode=True, default_flow_style=False)


def generate_benchmark_config(task_def: dict, output_dir: Path):
    """ç”Ÿæˆè©•ä¼°é…ç½®"""
    criteria = task_def.get('success_criteria', {})

    config = {
        'evaluation': {
            'test_file': 'data/test.jsonl',
            'metrics': {
                'primary': criteria.get('primary_metric', 'macro_f1'),
                'secondary': ['accuracy', 'per_class_f1'],
            },
            'thresholds': {
                'pass': {
                    criteria.get('primary_metric', 'macro_f1'): criteria.get('threshold', 0.80),
                },
            },
        },
        'output': {
            'results_dir': 'benchmarks/results/',
        },
    }

    config_path = output_dir / 'configs' / 'benchmark_config.yaml'
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, allow_unicode=True, default_flow_style=False)


def generate_validate_script(task_def: dict, output_dir: Path):
    """ç”Ÿæˆè³‡æ–™é©—è­‰è…³æœ¬"""
    output_format = task_def.get('output_format', {})
    labels = output_format.get('schema', {}).get(list(output_format.get('schema', {'label': {}}).keys())[0], {}).get('enum', ['æ­£é¢', 'è² é¢', 'ä¸­ç«‹'])

    content = f'''#!/usr/bin/env python3
"""
è³‡æ–™é©—è­‰è…³æœ¬
ä»»å‹™: {task_def['task_name']}
"""

import json
from pathlib import Path
from collections import Counter

EXPECTED_LABELS = {labels}

def validate_file(file_path: str) -> dict:
    """é©—è­‰å–®ä¸€æª”æ¡ˆ"""
    issues = []
    label_counts = Counter()

    with open(file_path, encoding='utf-8') as f:
        for i, line in enumerate(f, 1):
            try:
                item = json.loads(line)

                # æª¢æŸ¥å¿…è¦æ¬„ä½
                if 'text' not in item and 'input' not in item:
                    issues.append(f"Line {{i}}: Missing 'text' or 'input' field")

                if 'label' not in item and 'output' not in item:
                    issues.append(f"Line {{i}}: Missing 'label' or 'output' field")
                else:
                    label = item.get('label') or item.get('output')
                    label_counts[label] += 1

                    if label not in EXPECTED_LABELS:
                        issues.append(f"Line {{i}}: Invalid label '{{label}}'")

            except json.JSONDecodeError:
                issues.append(f"Line {{i}}: Invalid JSON")

    return {{
        'file': file_path,
        'total_lines': i,
        'issues': issues,
        'label_distribution': dict(label_counts),
    }}


def main():
    data_dir = Path('data')

    files_to_check = [
        data_dir / 'train.jsonl',
        data_dir / 'valid.jsonl',
        data_dir / 'test.jsonl',
    ]

    print("=" * 50)
    print("è³‡æ–™é©—è­‰å ±å‘Š")
    print("=" * 50)

    all_valid = True

    for file_path in files_to_check:
        if not file_path.exists():
            print(f"\\nâš ï¸  {{file_path}} ä¸å­˜åœ¨")
            continue

        result = validate_file(str(file_path))

        print(f"\\nğŸ“„ {{result['file']}}")
        print(f"   ç¸½ç­†æ•¸: {{result['total_lines']}}")
        print(f"   é¡åˆ¥åˆ†ä½ˆ: {{result['label_distribution']}}")

        if result['issues']:
            all_valid = False
            print(f"   âŒ ç™¼ç¾ {{len(result['issues'])}} å€‹å•é¡Œ:")
            for issue in result['issues'][:5]:
                print(f"      - {{issue}}")
            if len(result['issues']) > 5:
                print(f"      ... é‚„æœ‰ {{len(result['issues']) - 5}} å€‹å•é¡Œ")
        else:
            print("   âœ… é©—è­‰é€šé")

    print("\\n" + "=" * 50)
    if all_valid:
        print("âœ… æ‰€æœ‰æª”æ¡ˆé©—è­‰é€šé")
    else:
        print("âŒ éƒ¨åˆ†æª”æ¡ˆæœ‰å•é¡Œï¼Œè«‹ä¿®æ­£å¾Œé‡æ–°é©—è­‰")
    print("=" * 50)


if __name__ == "__main__":
    main()
'''

    script_path = output_dir / 'scripts' / '01_validate_data.py'
    script_path.write_text(content, encoding='utf-8')
    script_path.chmod(0o755)


def generate_run_pipeline(task_def: dict, output_dir: Path):
    """ç”Ÿæˆä¸€éµåŸ·è¡Œè…³æœ¬"""
    # å–å¾—ä¼ºæœå™¨é…ç½®ï¼ˆä½¿ç”¨è€…è¨­å®šæˆ–é è¨­å€¼ï¼‰
    server_config = task_def.get('server', {})
    server_host = server_config.get('host', '${SERVER_HOST}')
    ssh_key = server_config.get('ssh_key', '${SSH_KEY}')
    cuda_devices = server_config.get('cuda_devices', '${CUDA_DEVICES}')
    remote_path = server_config.get('remote_path', '~/tasks')

    content = f'''#!/bin/bash
# {task_def['task_name']} ä¸€éµåŸ·è¡Œè…³æœ¬

set -e

TASK_DIR="$(cd "$(dirname "$0")/.." && pwd)"
cd "$TASK_DIR"

echo "=========================================="
echo "ä»»å‹™: {task_def['task_name']}"
echo "ç›®éŒ„: $TASK_DIR"
echo "=========================================="

# Phase 3: è³‡æ–™æº–å‚™
echo ""
echo "=== Phase 3: è³‡æ–™æº–å‚™ ==="
python scripts/01_validate_data.py
python scripts/02_convert_format.py

# Phase 4: è¨“ç·´
echo ""
echo "=== Phase 4: è¨“ç·´æ¨¡å‹ ==="
if [ "$REMOTE" = "true" ]; then
    echo "ä½¿ç”¨é ç«¯è¨“ç·´..."

    # è®€å–ä¼ºæœå™¨è¨­å®šï¼ˆå¾ç’°å¢ƒè®Šæ•¸æˆ– task_definition.yamlï¼‰
    SERVER_HOST="${{SERVER_HOST:-{server_host}}}"
    SSH_KEY="${{SSH_KEY:-{ssh_key}}}"
    CUDA_DEVICES="${{CUDA_DEVICES:-{cuda_devices}}}"

    # æª¢æŸ¥å¿…è¦è¨­å®š
    if [ "$SERVER_HOST" = "${{SERVER_HOST}}" ] || [ -z "$SERVER_HOST" ]; then
        echo "âŒ éŒ¯èª¤: è«‹è¨­å®š SERVER_HOST ç’°å¢ƒè®Šæ•¸æˆ–åœ¨ task_definition.yaml ä¸­è¨­å®š server.host"
        echo "   ç¯„ä¾‹: export SERVER_HOST=user@your-gpu-server"
        exit 1
    fi

    # åŒæ­¥åˆ°é ç«¯
    rsync -avz -e "ssh -i $SSH_KEY" "$TASK_DIR" "$SERVER_HOST":{remote_path}/

    # é ç«¯åŸ·è¡Œ
    ssh -i "$SSH_KEY" "$SERVER_HOST" \\
        "cd {remote_path}/$(basename $TASK_DIR) && \\
         source ~/.venv/bin/activate && \\
         CUDA_VISIBLE_DEVICES=$CUDA_DEVICES python scripts/03_train.py"

    # åŒæ­¥çµæœ
    rsync -avz -e "ssh -i $SSH_KEY" "$SERVER_HOST":{remote_path}/$(basename $TASK_DIR)/models/ "$TASK_DIR/models/"
else
    python scripts/03_train.py
fi

# Phase 5: è©•ä¼°
echo ""
echo "=== Phase 5: è©•ä¼°æ•ˆèƒ½ ==="
python scripts/04_evaluate.py

echo ""
echo "=========================================="
echo "âœ… Pipeline å®Œæˆ"
echo "è©•ä¼°çµæœ: benchmarks/results/"
echo "=========================================="
'''

    script_path = output_dir / 'scripts' / 'run_pipeline.sh'
    script_path.write_text(content, encoding='utf-8')
    script_path.chmod(0o755)


def generate_integration_guide(task_def: dict, output_dir: Path):
    """ç”Ÿæˆæ•´åˆæŒ‡å—æ¨¡æ¿"""
    task_name = task_def['task_name']
    hf_config = task_def.get('huggingface', {})
    hf_org = hf_config.get('org', '{your-hf-org}')
    hf_prefix = hf_config.get('prefix', 'eland')

    content = f"""# {task_name} æ•´åˆæŒ‡å—

æœ¬æ–‡ä»¶æä¾› {task_name} æ¨¡å‹çš„æ•´åˆæŒ‡å—ï¼Œä¾›ä¸‹æ¸¸æœå‹™ä¸²æ¥ä½¿ç”¨ã€‚

## æ¨¡å‹è³‡è¨Š

| é …ç›® | èªªæ˜ |
|------|------|
| æ¨¡å‹åç¨± | {hf_prefix}-{task_name}-zh |
| åŸºç¤æ¨¡å‹ | {task_def.get('training', {}).get('base_model', 'Qwen/Qwen3-4B')} |
| æ”¯æ´èªè¨€ | {task_def.get('language', 'ç¹é«”ä¸­æ–‡')} |
| æˆæ¬Šæ¢æ¬¾ | Apache 2.0 |

## HuggingFace å„²å­˜åº«

| ç”¨é€” | é€£çµ |
|------|------|
| LoRA Adapter | https://huggingface.co/{hf_org}/{hf_prefix}-{task_name}-zh |
| vLLM éƒ¨ç½² | https://huggingface.co/{hf_org}/{hf_prefix}-{task_name}-zh-vllm |
| Ollama/GGUF | https://huggingface.co/{hf_org}/{hf_prefix}-{task_name}-zh-gguf |

## éƒ¨ç½²æ–¹å¼

### Ollama (æ¨è–¦æœ¬åœ°éƒ¨ç½²)

```bash
# ä¸‹è¼‰ä¸¦å»ºç«‹æ¨¡å‹
huggingface-cli download {hf_org}/{hf_prefix}-{task_name}-zh-gguf \\
    {hf_prefix}-{task_name}-zh-q8_0.gguf Modelfile --local-dir ./

ollama create {hf_prefix}-{task_name}-zh -f Modelfile
ollama run {hf_prefix}-{task_name}-zh "æ¸¬è©¦è¼¸å…¥"
```

### vLLM (ç”Ÿç”¢ç’°å¢ƒ)

```bash
vllm serve {hf_org}/{hf_prefix}-{task_name}-zh-vllm --port 8000
```

## ä½¿ç”¨ç¯„ä¾‹

```python
import requests

response = requests.post(
    "http://localhost:8000/v1/chat/completions",
    json={{
        "model": "{hf_prefix}-{task_name}-zh",
        "messages": [
            {{"role": "user", "content": "æ¸¬è©¦è¼¸å…¥"}}
        ]
    }}
)
print(response.json())
```

---

*ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d')}*
"""

    (output_dir / 'docs' / 'integration-guide.md').write_text(content, encoding='utf-8')


def main():
    parser = argparse.ArgumentParser(description='åˆå§‹åŒ– LLM è¨“ç·´å°ˆæ¡ˆ')
    parser.add_argument('--config', required=True, help='task_definition.yaml è·¯å¾‘')
    parser.add_argument('--output', required=True, help='è¼¸å‡ºç›®éŒ„')
    args = parser.parse_args()

    # è¼‰å…¥é…ç½®
    task_def = load_task_definition(args.config)
    output_dir = Path(args.output)

    print(f"åˆå§‹åŒ–å°ˆæ¡ˆ: {task_def['task_name']}")
    print(f"è¼¸å‡ºç›®éŒ„: {output_dir}")

    # å»ºç«‹ç›®éŒ„çµæ§‹
    create_directory_structure(output_dir)
    print("âœ… ç›®éŒ„çµæ§‹å·²å»ºç«‹")

    # è¤‡è£½ task_definition.yaml
    import shutil
    shutil.copy(args.config, output_dir / 'task_definition.yaml')
    print("âœ… task_definition.yaml å·²è¤‡è£½")

    # ç”Ÿæˆæª”æ¡ˆ
    generate_readme(task_def, output_dir)
    print("âœ… README.md å·²ç”Ÿæˆ")

    generate_training_config(task_def, output_dir)
    print("âœ… training_config.yaml å·²ç”Ÿæˆ")

    generate_benchmark_config(task_def, output_dir)
    print("âœ… benchmark_config.yaml å·²ç”Ÿæˆ")

    generate_validate_script(task_def, output_dir)
    print("âœ… 01_validate_data.py å·²ç”Ÿæˆ")

    generate_run_pipeline(task_def, output_dir)
    print("âœ… run_pipeline.sh å·²ç”Ÿæˆ")

    generate_integration_guide(task_def, output_dir)
    print("âœ… integration-guide.md å·²ç”Ÿæˆ")

    print()
    print("=" * 50)
    print(f"âœ… å°ˆæ¡ˆåˆå§‹åŒ–å®Œæˆ: {output_dir}")
    print()
    print("ä¸‹ä¸€æ­¥:")
    print(f"  1. å°‡è³‡æ–™æ”¾å…¥ {output_dir}/data/")
    print(f"  2. åŸ·è¡Œ python {output_dir}/scripts/01_validate_data.py")
    print(f"  3. åŸ·è¡Œ {output_dir}/scripts/run_pipeline.sh")
    print("=" * 50)


if __name__ == "__main__":
    main()
'''

    script_path = output_dir / 'scripts' / 'init_project.py'
    script_path.write_text(content, encoding='utf-8')
    script_path.chmod(0o755)
