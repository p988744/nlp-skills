#!/usr/bin/env python3
"""
{{ task_name }} 訓練腳本

自動生成，可根據需要調整。
"""

import json
import torch
from pathlib import Path
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import Dataset


def load_data(file_path: str) -> Dataset:
    """載入 JSONL 資料"""
    data = []
    with open(file_path, encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    return Dataset.from_list(data)


def format_chat(example, tokenizer):
    """格式化為 chat format"""
    text = tokenizer.apply_chat_template(
        example['messages'],
        tokenize=False,
        add_generation_prompt=False,
        {% if model_family == 'qwen3' %}
        enable_thinking=False,  # Qwen3 關閉思考模式
        {% endif %}
    )
    return {'text': text}


def main():
    # ========== 配置 ==========
    BASE_MODEL = "{{ base_model }}"
    OUTPUT_DIR = "models/adapter"

    LORA_CONFIG = {
        'r': {{ lora_r }},
        'lora_alpha': {{ lora_alpha }},
        'lora_dropout': {{ lora_dropout }},
        'target_modules': {{ target_modules }},
        'bias': 'none',
        'task_type': 'CAUSAL_LM',
    }

    TRAINING_ARGS = {
        'output_dir': OUTPUT_DIR,
        'num_train_epochs': {{ epochs }},
        'per_device_train_batch_size': {{ batch_size }},
        'gradient_accumulation_steps': {{ gradient_accumulation }},
        'learning_rate': {{ learning_rate }},
        'warmup_ratio': 0.1,
        'weight_decay': 0.01,
        'logging_steps': 10,
        'save_strategy': 'epoch',
        'evaluation_strategy': 'epoch',
        'load_best_model_at_end': True,
        'bf16': True,
        'max_seq_length': {{ max_seq_length }},
    }

    # ========== 載入模型 ==========
    print(f"載入模型: {BASE_MODEL}")

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.bfloat16,
        device_map='auto',
        trust_remote_code=True,
    )

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    tokenizer.pad_token = tokenizer.eos_token

    # ========== LoRA 配置 ==========
    print("配置 LoRA...")
    lora_config = LoraConfig(**LORA_CONFIG)
    model = get_peft_model(model, lora_config)

    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"可訓練參數: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")

    # ========== 載入資料 ==========
    print("載入訓練資料...")
    train_data = load_data('data/chat_format/train.jsonl')
    eval_data = load_data('data/chat_format/valid.jsonl')

    train_data = train_data.map(lambda x: format_chat(x, tokenizer))
    eval_data = eval_data.map(lambda x: format_chat(x, tokenizer))

    print(f"訓練樣本: {len(train_data)}")
    print(f"驗證樣本: {len(eval_data)}")

    # ========== 訓練 ==========
    print("開始訓練...")
    training_args = TrainingArguments(**TRAINING_ARGS)

    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_data,
        eval_dataset=eval_data,
        tokenizer=tokenizer,
        dataset_text_field='text',
    )

    trainer.train()

    # ========== 儲存 ==========
    print(f"儲存模型到: {OUTPUT_DIR}")
    trainer.save_model()
    tokenizer.save_pretrained(OUTPUT_DIR)

    print("✅ 訓練完成!")


if __name__ == "__main__":
    main()
