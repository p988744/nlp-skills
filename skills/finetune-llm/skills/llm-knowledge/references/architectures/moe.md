# Mixture of Experts (MoE) 架構

## 概述

MoE 架構將模型分成多個專家網路，每次推理只激活部分專家，實現「大模型效能、小模型成本」。

**2025 年起，幾乎所有頂尖開源模型都採用 MoE 架構。**

## 架構原理

```
輸入 → Router (門控) → 選擇 Top-K 專家 → 加權輸出
              │
    ┌─────────┼─────────┐
    ▼         ▼         ▼
 Expert 1  Expert 2 ... Expert N
    │         │         │
    └─────────┴─────────┘
              │
         加權合併
              │
              ▼
            輸出
```

**關鍵**: 每次只激活 K 個專家（通常 K=2）

## 代表模型

| 模型 | 總參數 | 激活參數 | 專家數 |
|------|--------|----------|--------|
| **DeepSeek-V3** | 671B | 37B | 256/層 |
| **DeepSeek-R1** | 671B | 37B | 256/層 |
| **Qwen3-235B** | 235B | 22B | MoE |
| **Qwen3-30B** | 30B | 3B | MoE |
| **Mixtral 8x22B** | 141B | 39B | 8 |

## 優點

✅ **推理高效**: 只計算部分專家
✅ **效能強大**: 總參數量大，知識容量高
✅ **成本可控**: 激活參數少，VRAM 需求相對低
✅ **訓練效率**: DeepSeek-V3 訓練成本僅 $5.6M

## 缺點

❌ **訓練複雜**: 負載均衡、專家坍塌問題
❌ **工具支援**: 部分框架支援有限
❌ **微調困難**: 需要特殊處理
❌ **記憶體特性**: 需載入所有專家權重

## DeepSeek MoE 創新

### 1. 細粒度專家

```
傳統 MoE: N 個專家，每個維度 d
DeepSeek: mN 個專家，每個維度 d/m
→ 更細的知識分解，專家更專精
```

### 2. 共享專家

```
傳統: 所有專家平等
DeepSeek: 部分專家為「共享專家」
→ 學習通用知識，其他專家更專精
```

### 3. 輔助損失-free 負載均衡

- 不需要額外損失項來平衡專家使用
- 訓練更穩定

### 4. MLA (Multi-head Latent Attention)

```
傳統 MHA: 獨立的 Q, K, V
MLA: 低秩壓縮的 K, V
→ 減少 KV Cache，長序列更高效
```

## VRAM 需求

| 模型 | 推理 (FP16) | 推理 (INT4) |
|------|-------------|-------------|
| Mixtral 8x7B | 100GB | 26GB |
| Qwen3-30B | 60GB | 16GB |
| DeepSeek-V3 | 400GB+ | 100GB+ |

## 微調注意事項

### 挑戰

1. **負載均衡**: 訓練時專家使用需均衡
2. **專家坍塌**: 某些專家可能不被使用
3. **記憶體**: 需載入所有專家

### 建議配置

```yaml
# MoE 模型 LoRA 配置
lora:
  r: 16  # 較小的 rank
  lora_alpha: 32
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    # 注意：通常不對專家 FFN 做 LoRA

training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  # 較小的 batch size 因為記憶體需求
```

### 工具支援

| 框架 | MoE 支援 |
|------|----------|
| Transformers | ✅ 基本支援 |
| vLLM | ✅ 良好支援 |
| Ollama | ⚠️ 部分模型 |
| LLaMA-Factory | ⚠️ 有限 |

## 適用場景

| 場景 | 是否適合 | 說明 |
|------|----------|------|
| 推理任務 | ✅ | DeepSeek-R1 專為推理設計 |
| 程式碼 | ✅ | DeepSeek Coder V2 |
| 高吞吐量 | ✅ | 推理成本低 |
| 資源受限微調 | ❌ | 需要完整載入 |
| 邊緣部署 | ❌ | 模型太大 |

## 訓練成本對比

| 模型 | GPU 時數 | 估計成本 |
|------|----------|----------|
| DeepSeek-V3 | 2.8M H800 | ~$5.6M |
| Llama 3.1 405B | 30.8M | ~$60M+ |

→ DeepSeek 訓練效率約 **11 倍**

## 相關

- [dense.md](dense.md) - 傳統架構，更適合微調
- [models/deepseek.md](../models/deepseek.md) - DeepSeek 模型詳情

---

*更新: 2026-01*
