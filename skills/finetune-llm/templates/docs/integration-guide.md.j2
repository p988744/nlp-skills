# {{ task_name }} 整合指南

本文件提供 {{ task_name }} 模型的整合指南，供下游服務串接使用。

## 模型資訊

| 項目 | 說明 |
|------|------|
| 模型名稱 | eland-{{ task_name }}-zh |
| 基礎模型 | {{ base_model }} |
| 支援語言 | {{ language }} |
| 任務類型 | {{ task_type }} |
| 授權條款 | Apache 2.0 |

## 效能指標

| 指標 | 分數 |
|------|------|
{% for metric, score in metrics.items() %}
| {{ metric }} | {{ "%.2f%%"|format(score * 100) }} |
{% endfor %}

## HuggingFace 儲存庫

| 用途 | 連結 |
|------|------|
| LoRA Adapter | https://huggingface.co/{{ hf_org }}/eland-{{ task_name }}-zh |
| vLLM 部署 | https://huggingface.co/{{ hf_org }}/eland-{{ task_name }}-zh-vllm |
| Ollama/GGUF | https://huggingface.co/{{ hf_org }}/eland-{{ task_name }}-zh-gguf |

## 部署方式

### Ollama (本地部署推薦)

```bash
# 下載並建立模型
huggingface-cli download {{ hf_org }}/eland-{{ task_name }}-zh-gguf \
    eland-{{ task_name }}-zh-q8_0.gguf Modelfile --local-dir ./

ollama create eland-{{ task_name }}-zh -f Modelfile
ollama run eland-{{ task_name }}-zh "測試輸入"
```

### vLLM (生產環境)

```bash
vllm serve {{ hf_org }}/eland-{{ task_name }}-zh-vllm --port 8000
```

## API 使用範例

### Python

```python
import requests

response = requests.post(
    "http://localhost:8000/v1/chat/completions",
    json={
        "model": "eland-{{ task_name }}-zh",
        "messages": [
            {"role": "system", "content": "{{ system_prompt }}"},
            {"role": "user", "content": "測試輸入"}
        ]
    }
)
result = response.json()
print(result['choices'][0]['message']['content'])
```

### cURL

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "eland-{{ task_name }}-zh",
    "messages": [
      {"role": "system", "content": "{{ system_prompt }}"},
      {"role": "user", "content": "測試輸入"}
    ]
  }'
```

## 輸入輸出格式

### 輸入

{{ input_format_description }}

### 輸出

{% if task_type == 'classification' %}
以下類別之一：
{% for label in labels %}
- {{ label }}
{% endfor %}
{% else %}
{{ output_format_description }}
{% endif %}

## 注意事項

{% if model_family == 'qwen3' %}
### Qwen3 Thinking Mode

使用 Transformers 或 vLLM 時，需關閉思考模式：

```python
prompt = tokenizer.apply_chat_template(
    messages,
    enable_thinking=False  # 重要！
)
```
{% endif %}

### System Prompt

為獲得最佳效果，建議使用以下 system prompt：

```
{{ system_prompt }}
```

---

*生成時間: {{ generated_date }}*
